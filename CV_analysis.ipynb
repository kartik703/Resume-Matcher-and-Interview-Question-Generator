{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4gMKZtVdWJk+sI5NwdnQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartik703/Resume-Matcher-and-Interview-Question-Generator/blob/main/CV_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the app with ngrok\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "print(\"Public URL:\", public_url)\n",
        "app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "ZarUH4jcknco",
        "outputId": "88b47a00-9d03-4317-f5c7-5ecb66d0f0d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyngrok'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2c2e82d36ce9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run the app with ngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublic_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Public URL:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyngrok'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain faiss-cpu transformers sentence-transformers PyPDF2 pyngrok Flask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UasAQqOqiZXK",
        "outputId": "1c0b0f23-58c0-4e00-d343-bcaf6d558358"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.41)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2, pyngrok, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed PyPDF2-3.0.1 faiss-cpu-1.10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyngrok-7.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok authtoken\n",
        "ngrok.set_auth_token(\"2spSTpuhUshKKVq7XVFQbeG2ejK_5pFx4CNCK9tnex9ZSER5y\")\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "print(\"Public URL:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnbSGM73qx_x",
        "outputId": "7e735e71-3716-4593-b880-8892d7486048"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://5d74-34-73-61-222.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install langchain faiss-cpu transformers sentence-transformers PyPDF2 pyngrok Flask\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "from flask import Flask, request, jsonify, render_template\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import base64\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Create the templates folder and index.html file\n",
        "os.makedirs(\"templates\", exist_ok=True)\n",
        "index_html_content = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Resume Matcher</title>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Resume Matcher and Interview Question Generator</h1>\n",
        "    <form id=\"resumeForm\">\n",
        "        <label for=\"job_description\">Job Description:</label><br>\n",
        "        <textarea id=\"job_description\" name=\"job_description\" rows=\"4\" cols=\"50\"></textarea><br><br>\n",
        "        <label for=\"resume_file\">Upload Resume (PDF):</label><br>\n",
        "        <input type=\"file\" id=\"resume_file\" name=\"resume_file\" accept=\".pdf\"><br><br>\n",
        "        <button type=\"submit\">Process</button>\n",
        "    </form>\n",
        "    <h2>Results</h2>\n",
        "    <div id=\"results\"></div>\n",
        "\n",
        "    <script>\n",
        "        document.getElementById(\"resumeForm\").addEventListener(\"submit\", async (e) => {\n",
        "            e.preventDefault();\n",
        "            const jobDescription = document.getElementById(\"job_description\").value;\n",
        "            const resumeFile = document.getElementById(\"resume_file\").files[0];\n",
        "\n",
        "            if (!resumeFile) {\n",
        "                alert(\"Please upload a resume file\");\n",
        "                return;\n",
        "            }\n",
        "\n",
        "            const reader = new FileReader();\n",
        "            reader.onload = async () => {\n",
        "                const base64Resume = reader.result.split(\",\")[1];\n",
        "                try {\n",
        "                    const response = await fetch(\"/process\", {\n",
        "                        method: \"POST\",\n",
        "                        headers: {\n",
        "                            \"Content-Type\": \"application/json\",\n",
        "                        },\n",
        "                        body: JSON.stringify({\n",
        "                            job_description: jobDescription,\n",
        "                            resume_file: base64Resume,\n",
        "                        }),\n",
        "                    });\n",
        "                    const data = await response.json();\n",
        "                    if (data.error) throw new Error(data.error);\n",
        "\n",
        "                    document.getElementById(\"results\").innerHTML = `\n",
        "                        <h3>Match Summary:</h3>\n",
        "                        <p>${data.match_summary}</p>\n",
        "                        <h3>Interview Questions:</h3>\n",
        "                        <ul>${data.interview_questions.split('\\n').map(q => q ? `<li>${q}</li>` : '').join('')}</ul>\n",
        "                    `;\n",
        "                } catch (error) {\n",
        "                    document.getElementById(\"results\").innerHTML = `<p style=\"color: red\">Error: ${error.message}</p>`;\n",
        "                }\n",
        "            };\n",
        "            reader.readAsDataURL(resumeFile);\n",
        "        });\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "with open(\"templates/index.html\", \"w\") as f:\n",
        "    f.write(index_html_content)\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load Hugging Face LLM\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "llm_pipeline = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=512\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "\n",
        "# Load embeddings model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def parse_resume(file_path):\n",
        "    \"\"\"Extract text from a resume (PDF only)\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise ValueError(f\"File path {file_path} does not exist.\")\n",
        "    if not file_path.lower().endswith(\".pdf\"):\n",
        "        raise ValueError(\"Only PDF files are supported.\")\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    pages = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    return text_splitter.split_documents(pages)\n",
        "\n",
        "def create_vector_store(resume_chunks):\n",
        "    \"\"\"Create FAISS vector store from extracted resume text\"\"\"\n",
        "    return FAISS.from_documents(resume_chunks, embeddings)\n",
        "\n",
        "def job_match(job_description, vector_store):\n",
        "    \"\"\"Find best matching resumes for a given job description\"\"\"\n",
        "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "    # Custom prompt template\n",
        "    prompt_template = \"\"\"Answer the question based on the resume context below.\n",
        "    Summarize how the candidate's qualifications match the job description.\n",
        "    Focus on technical skills, relevant experience, and education.\n",
        "\n",
        "    Job Description: {job_desc}\n",
        "    Context: {context}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"job_desc\", \"context\"]\n",
        "    )\n",
        "\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": PROMPT}\n",
        "    )\n",
        "\n",
        "    result = qa.invoke({\"input_documents\": [], \"query\": job_description})\n",
        "    return result[\"result\"], result[\"source_documents\"]\n",
        "\n",
        "def generate_interview_questions(full_resume_text):\n",
        "    \"\"\"Generate interview questions based on a candidate's resume\"\"\"\n",
        "    # Tokenize and chunk the resume text\n",
        "    tokens = tokenizer.encode(full_resume_text)\n",
        "    max_tokens = 500\n",
        "    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
        "\n",
        "    questions = []\n",
        "    for chunk in chunks:\n",
        "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
        "        prompt = f\"\"\"Generate 2 professional interview questions based on:\n",
        "        {chunk_text}\n",
        "        Focus on technical skills, projects, and work experience.\n",
        "        Format each question on a new line.\"\"\"\n",
        "\n",
        "        response = llm_pipeline(\n",
        "            prompt,\n",
        "            max_length=512,\n",
        "            num_return_sequences=1,\n",
        "            truncation=True\n",
        "        )\n",
        "        if response and len(response) > 0:\n",
        "            questions.extend(response[0]['generated_text'].split('\\n'))\n",
        "\n",
        "    # Clean and deduplicate questions\n",
        "    clean_questions = [q.strip() for q in questions if q.strip()]\n",
        "    unique_questions = list(set(clean_questions))\n",
        "    return '\\n'.join(unique_questions[:5])\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\"])\n",
        "def home():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/process\", methods=[\"POST\"])\n",
        "def process():\n",
        "    try:\n",
        "        data = request.json\n",
        "        if not data:\n",
        "            return jsonify({\"error\": \"No data provided\"}), 400\n",
        "\n",
        "        job_desc = data.get(\"job_description\")\n",
        "        resume_file = data.get(\"resume_file\")\n",
        "\n",
        "        if not job_desc or not resume_file:\n",
        "            return jsonify({\"error\": \"Missing job_description or resume_file\"}), 400\n",
        "\n",
        "        # Save and process the uploaded file\n",
        "        temp_file = \"temp_resume.pdf\"\n",
        "        try:\n",
        "            with open(temp_file, \"wb\") as f:\n",
        "                f.write(base64.b64decode(resume_file.encode(\"utf-8\")))\n",
        "\n",
        "            # Process resume\n",
        "            resume_chunks = parse_resume(temp_file)\n",
        "            if not resume_chunks:\n",
        "                raise ValueError(\"No content found in resume\")\n",
        "\n",
        "            vector_store = create_vector_store(resume_chunks)\n",
        "            match_summary, source_docs = job_match(job_desc, vector_store)\n",
        "\n",
        "            # Use full resume text for questions\n",
        "            full_resume_text = \" \".join([doc.page_content for doc in resume_chunks])\n",
        "            interview_questions = generate_interview_questions(full_resume_text)\n",
        "\n",
        "            return jsonify({\n",
        "                \"match_summary\": match_summary,\n",
        "                \"interview_questions\": interview_questions\n",
        "            })\n",
        "        finally:\n",
        "            if os.path.exists(temp_file):\n",
        "                os.remove(temp_file)\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# Run the app with ngrok\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "print(\"Public URL:\", public_url)\n",
        "app.run(debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCNlSyaXq18M",
        "outputId": "670a59e5-f066-4d8f-8982-1295267bb524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.41)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://0296-34-73-61-222.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2025 00:24:50] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2025 00:26:01] \"GET /?job_description=About+Zelp%0D%0AZELP+is+a+London-based+start-up+dedicated+to+mitigating+the+impact+of+the+livestock+industry+on+the+environment.+It+was+founded+in+2017+by+two+brothers+motivated+by+the+global+need+to+address+the+climate+crisis+and+the+urge+to+make+the+beef+and+dairy+industry+more+sustainable.+%0D%0A%0D%0AZELP+has+patented+a+wearable+device+for+cattle+that+neutralizes+livestock’s+emissions,+which+are+responsible+for+almost+10%25+of+the+total+greenhouse+gas+emissions+that+cause+global+warming.+The+deployment+of+our+device+on+a+global+scale+has+the+potential+to+reduce+greenhouse+gas+emissions+equivalent+to+eliminating+emissions+from+all+forms+of+transportation+combined,+and+to+transform+the+beef+and+dairy+industry+by+empowering+consumers+to+make+their+diets+more+sustainable.%0D%0A%0D%0AAbout+the+role%0D%0AZELP+is+seeking+a+highly+motivated+and+skilled+Data+Scientist+to+join+our+growing+team.+The+ideal+candidate+will+possess+a+strong+technical+foundation+in+data+analysis,+machine+learning,+and+statistical+modeling,+coupled+with+excellent+communication+and+collaboration+skills.+The+Data+Scientist+will+play+a+key+role+in+extracting+actionable+insights+from+complex+datasets,+driving+data-driven+decision-making,+and+contributing+to+the+development+of+innovative+solutions+that+address+critical+business+challenges.%0D%0A%0D%0AKey+Responsibilities:%0D%0ACollaborate+with+stakeholders+to+identify+and+understand+business+needs,+translating+them+into+data-driven+solutions.%0D%0ADesign,+implement,+and+maintain+robust+data+pipelines+for+collecting,+cleaning,+transforming,+and+analyzing+large+datasets.%0D%0APerform+exploratory+data+analysis+(EDA)+to+uncover+trends+and+patterns.%0D%0ADevelop+and+apply+advanced+statistical+and+machine+learning+models+to+solve+business+problems,+including+forecasting,+segmentation,+optimization,+and+predictive+modeling.%0D%0AConduct+rigorous+data+analysis,+interpret+results,+and+communicate+findings+to+technical+and+non-technical+audiences+in+a+clear+and+compelling+manner.%0D%0ADevelop+and+maintain+interactive+dashboards+and+visualizations+to+effectively+present+data+insights+and+support+decision-making.%0D%0AStay+abreast+of+emerging+technologies+and+industry+best+practices+in+data+science,+continuously+exploring+opportunities+for+innovation+and+improvement.%0D%0AContribute+to+the+development+of+a+data-driven+culture+within+the+organization,+promoting+the+value+of+data-informed+decision-making.%0D%0A%0D%0ARequired+Skills+and+Experience:%0D%0A4%2B+years+of+experience+in+data+science,+analytics,+or+a+related+field.%0D%0AProven+experience+working+with+large+datasets+and+statistical+modeling.%0D%0AProficiency+in+programming+languages+such+as+Python+or+R,+with+a+strong+understanding+of+object-oriented+programming+concepts.%0D%0AExperience+in+designing+and+implementing+ETL+pipelines,+data+warehousing,+and+data+modeling.%0D%0AExperience+with+machine+learning+frameworks+(e.g.,+TensorFlow,+PyTorch,+scikit-learn).%0D%0AExpertise+in+statistical+modeling,+machine+learning+algorithms,+and+data+mining+techniques.%0D%0AFamiliarity+with+data+visualization+and+reporting+tools,+such+as+Tableau,+Power+BI,+or+Looker.%0D%0AFamiliarity+with+cloud+platforms+(e.g.,+AWS,+Azure,+GCP)+is+a+plus.%0D%0AStrong+analytical+and+problem-solving+skills,+with+the+ability+to+translate+complex+data+into+actionable+insights.%0D%0AExcellent+communication+and+presentation+skills,+with+the+ability+to+effectively+communicate+with+both+technical+and+non-technical+audiences.%0D%0ACollaborative+mindset+and+ability+to+work+effectively+within+cross-functional+teams.%0D%0A%0D%0ACompany+Benefits%0D%0AFlexible+hours%0D%0APension+scheme%0D%0AFree+coffee,+fruit,+breakfast+and+snacks%0D%0AYoga+Sessions%0D%0A%0D%0AInterview+Process%0D%0AIntroductory+Call+with+Hiring+Manager%0D%0ATechnical+interview%0D%0AShort+take-home+exercise%0D%0AHR+interview%0D%0AOffer%0D%0A&resume_file=Kartik+Goswami.pdf HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2025 00:26:11] \"GET /?job_description=About+Zelp%0D%0AZELP+is+a+London-based+start-up+dedicated+to+mitigating+the+impact+of+the+livestock+industry+on+the+environment.+It+was+founded+in+2017+by+two+brothers+motivated+by+the+global+need+to+address+the+climate+crisis+and+the+urge+to+make+the+beef+and+dairy+industry+more+sustainable.+%0D%0A%0D%0AZELP+has+patented+a+wearable+device+for+cattle+that+neutralizes+livestock’s+emissions,+which+are+responsible+for+almost+10%25+of+the+total+greenhouse+gas+emissions+that+cause+global+warming.+The+deployment+of+our+device+on+a+global+scale+has+the+potential+to+reduce+greenhouse+gas+emissions+equivalent+to+eliminating+emissions+from+all+forms+of+transportation+combined,+and+to+transform+the+beef+and+dairy+industry+by+empowering+consumers+to+make+their+diets+more+sustainable.%0D%0A%0D%0AAbout+the+role%0D%0AZELP+is+seeking+a+highly+motivated+and+skilled+Data+Scientist+to+join+our+growing+team.+The+ideal+candidate+will+possess+a+strong+technical+foundation+in+data+analysis,+machine+learning,+and+statistical+modeling,+coupled+with+excellent+communication+and+collaboration+skills.+The+Data+Scientist+will+play+a+key+role+in+extracting+actionable+insights+from+complex+datasets,+driving+data-driven+decision-making,+and+contributing+to+the+development+of+innovative+solutions+that+address+critical+business+challenges.%0D%0A%0D%0AKey+Responsibilities:%0D%0ACollaborate+with+stakeholders+to+identify+and+understand+business+needs,+translating+them+into+data-driven+solutions.%0D%0ADesign,+implement,+and+maintain+robust+data+pipelines+for+collecting,+cleaning,+transforming,+and+analyzing+large+datasets.%0D%0APerform+exploratory+data+analysis+(EDA)+to+uncover+trends+and+patterns.%0D%0ADevelop+and+apply+advanced+statistical+and+machine+learning+models+to+solve+business+problems,+including+forecasting,+segmentation,+optimization,+and+predictive+modeling.%0D%0AConduct+rigorous+data+analysis,+interpret+results,+and+communicate+findings+to+technical+and+non-technical+audiences+in+a+clear+and+compelling+manner.%0D%0ADevelop+and+maintain+interactive+dashboards+and+visualizations+to+effectively+present+data+insights+and+support+decision-making.%0D%0AStay+abreast+of+emerging+technologies+and+industry+best+practices+in+data+science,+continuously+exploring+opportunities+for+innovation+and+improvement.%0D%0AContribute+to+the+development+of+a+data-driven+culture+within+the+organization,+promoting+the+value+of+data-informed+decision-making.%0D%0A%0D%0ARequired+Skills+and+Experience:%0D%0A4%2B+years+of+experience+in+data+science,+analytics,+or+a+related+field.%0D%0AProven+experience+working+with+large+datasets+and+statistical+modeling.%0D%0AProficiency+in+programming+languages+such+as+Python+or+R,+with+a+strong+understanding+of+object-oriented+programming+concepts.%0D%0AExperience+in+designing+and+implementing+ETL+pipelines,+data+warehousing,+and+data+modeling.%0D%0AExperience+with+machine+learning+frameworks+(e.g.,+TensorFlow,+PyTorch,+scikit-learn).%0D%0AExpertise+in+statistical+modeling,+machine+learning+algorithms,+and+data+mining+techniques.%0D%0AFamiliarity+with+data+visualization+and+reporting+tools,+such+as+Tableau,+Power+BI,+or+Looker.%0D%0AFamiliarity+with+cloud+platforms+(e.g.,+AWS,+Azure,+GCP)+is+a+plus.%0D%0AStrong+analytical+and+problem-solving+skills,+with+the+ability+to+translate+complex+data+into+actionable+insights.%0D%0AExcellent+communication+and+presentation+skills,+with+the+ability+to+effectively+communicate+with+both+technical+and+non-technical+audiences.%0D%0ACollaborative+mindset+and+ability+to+work+effectively+within+cross-functional+teams.%0D%0A%0D%0ACompany+Benefits%0D%0AFlexible+hours%0D%0APension+scheme%0D%0AFree+coffee,+fruit,+breakfast+and+snacks%0D%0AYoga+Sessions%0D%0A%0D%0AInterview+Process%0D%0AIntroductory+Call+with+Hiring+Manager%0D%0ATechnical+interview%0D%0AShort+take-home+exercise%0D%0AHR+interview%0D%0AOffer%0D%0A&resume_file=part+time+job+resume.pdf HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2025 00:26:41] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2025 00:26:52] \"GET /?job_description=About+Zelp%0D%0AZELP+is+a+London-based+start-up+dedicated+to+mitigating+the+impact+of+the+livestock+industry+on+the+environment.+It+was+founded+in+2017+by+two+brothers+motivated+by+the+global+need+to+address+the+climate+crisis+and+the+urge+to+make+the+beef+and+dairy+industry+more+sustainable.+%0D%0A%0D%0AZELP+has+patented+a+wearable+device+for+cattle+that+neutralizes+livestock’s+emissions,+which+are+responsible+for+almost+10%25+of+the+total+greenhouse+gas+emissions+that+cause+global+warming.+The+deployment+of+our+device+on+a+global+scale+has+the+potential+to+reduce+greenhouse+gas+emissions+equivalent+to+eliminating+emissions+from+all+forms+of+transportation+combined,+and+to+transform+the+beef+and+dairy+industry+by+empowering+consumers+to+make+their+diets+more+sustainable.%0D%0A%0D%0AAbout+the+role%0D%0AZELP+is+seeking+a+highly+motivated+and+skilled+Data+Scientist+to+join+our+growing+team.+The+ideal+candidate+will+possess+a+strong+technical+foundation+in+data+analysis,+machine+learning,+and+statistical+modeling,+coupled+with+excellent+communication+and+collaboration+skills.+The+Data+Scientist+will+play+a+key+role+in+extracting+actionable+insights+from+complex+datasets,+driving+data-driven+decision-making,+and+contributing+to+the+development+of+innovative+solutions+that+address+critical+business+challenges.%0D%0A%0D%0AKey+Responsibilities:%0D%0ACollaborate+with+stakeholders+to+identify+and+understand+business+needs,+translating+them+into+data-driven+solutions.%0D%0ADesign,+implement,+and+maintain+robust+data+pipelines+for+collecting,+cleaning,+transforming,+and+analyzing+large+datasets.%0D%0APerform+exploratory+data+analysis+(EDA)+to+uncover+trends+and+patterns.%0D%0ADevelop+and+apply+advanced+statistical+and+machine+learning+models+to+solve+business+problems,+including+forecasting,+segmentation,+optimization,+and+predictive+modeling.%0D%0AConduct+rigorous+data+analysis,+interpret+results,+and+communicate+findings+to+technical+and+non-technical+audiences+in+a+clear+and+compelling+manner.%0D%0ADevelop+and+maintain+interactive+dashboards+and+visualizations+to+effectively+present+data+insights+and+support+decision-making.%0D%0AStay+abreast+of+emerging+technologies+and+industry+best+practices+in+data+science,+continuously+exploring+opportunities+for+innovation+and+improvement.%0D%0AContribute+to+the+development+of+a+data-driven+culture+within+the+organization,+promoting+the+value+of+data-informed+decision-making.%0D%0A%0D%0ARequired+Skills+and+Experience:%0D%0A4%2B+years+of+experience+in+data+science,+analytics,+or+a+related+field.%0D%0AProven+experience+working+with+large+datasets+and+statistical+modeling.%0D%0AProficiency+in+programming+languages+such+as+Python+or+R,+with+a+strong+understanding+of+object-oriented+programming+concepts.%0D%0AExperience+in+designing+and+implementing+ETL+pipelines,+data+warehousing,+and+data+modeling.%0D%0AExperience+with+machine+learning+frameworks+(e.g.,+TensorFlow,+PyTorch,+scikit-learn).%0D%0AExpertise+in+statistical+modeling,+machine+learning+algorithms,+and+data+mining+techniques.%0D%0AFamiliarity+with+data+visualization+and+reporting+tools,+such+as+Tableau,+Power+BI,+or+Looker.%0D%0AFamiliarity+with+cloud+platforms+(e.g.,+AWS,+Azure,+GCP)+is+a+plus.%0D%0AStrong+analytical+and+problem-solving+skills,+with+the+ability+to+translate+complex+data+into+actionable+insights.%0D%0AExcellent+communication+and+presentation+skills,+with+the+ability+to+effectively+communicate+with+both+technical+and+non-technical+audiences.%0D%0ACollaborative+mindset+and+ability+to+work+effectively+within+cross-functional+teams.%0D%0A%0D%0ACompany+Benefits%0D%0AFlexible+hours%0D%0APension+scheme%0D%0AFree+coffee,+fruit,+breakfast+and+snacks%0D%0AYoga+Sessions%0D%0A%0D%0AInterview+Process%0D%0AIntroductory+Call+with+Hiring+Manager%0D%0ATechnical+interview%0D%0AShort+take-home+exercise%0D%0AHR+interview%0D%0AOffer%0D%0A&resume_file=Kartik+Goswami.pdf HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "ZSArKgMyr2mo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f8d7c37-1751-4bf8-e33c-69c36bb2368c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.41)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.20)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.11)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain-community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.19 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jOmNqAerjBzN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}